{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b64c8b-72f6-4eb9-8c12-7c5ea9504273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import multiprocessing\n",
    "import math\n",
    "import json\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from scipy.spatial.distance import cdist\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (silhouette_score,\n",
    "                             davies_bouldin_score)\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from keybert import KeyBERT\n",
    "\n",
    "from src.model import (get_model,\n",
    "                       generate_video_embeddings)\n",
    "from src.dataset import (VideoFrameDataset,\n",
    "                         load_MSRVTT_dataset)\n",
    "from src.visualization import (visualize_random_files,\n",
    "                               get_numpy_images,\n",
    "                               visualize_frames,\n",
    "                               plot_clusters,\n",
    "                               plot_count_plot)\n",
    "from src.utils import (get_video_infos,\n",
    "                       show_dataset_stats,\n",
    "                       read_embeddings_and_aggregate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca6444a-864e-46df-86e2-f1c935408ff4",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14445b-769e-4272-ae15-a2438e9f78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    random_seed: int = 28\n",
    "    vis_n_samples: int = 9\n",
    "    vis_n_cols: int = 3\n",
    "    vis_width: int = 320\n",
    "    img_h: int = 224\n",
    "    img_w: int = 224\n",
    "    frame_interval: int = 10\n",
    "    norm_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    norm_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "    model_type = 'openclip_b32'\n",
    "    batch_size: int = 128\n",
    "    n_workers: int = multiprocessing.cpu_count()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agg_strategy: str = 'mean'\n",
    "    n_clusters: int = 20\n",
    "    tsne_n_components: int = 2\n",
    "    summarizer_max_length: int = 100\n",
    "    summarizer_min_length: int = 20\n",
    "    kw_k_nearest: int = 20\n",
    "    kw_score_threshold: float = 0.2\n",
    "    kw_top_n: int = 10\n",
    "\n",
    "config = Config()\n",
    "random.seed(config.random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de97659-6166-43bc-8443-5a291bc509b1",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff5ec8-f8b6-4f4a-ba6e-ceb99565e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset was downloaded from https://cove.thecvf.com/datasets/839\n",
    "dataset_path = Path('/datasets/video_clustering/MSRVTT/')\n",
    "msrvtt_dataset = load_MSRVTT_dataset(dataset_path)\n",
    "\n",
    "save_dir = dataset_path / 'results'\n",
    "save_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a7d3c-7bee-40c3-a81c-877409db0baf",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff2433-7a3f-4fce-89ca-1a3075d8f0ca",
   "metadata": {},
   "source": [
    "## Dataset analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56597dd-7dab-4f5a-9284-63340aa047bc",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21319349-abde-471d-a8c4-96422b6e3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_infos = get_video_infos(msrvtt_dataset.train_video_paths)\n",
    "show_dataset_stats(train_video_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b8b67-0b87-4c17-96bd-8b73de26ebab",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937d3a2-86b9-409b-a904-5b7fd69ce275",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_infos = get_video_infos(msrvtt_dataset.test_video_paths)\n",
    "show_dataset_stats(test_video_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3fb23-8770-403f-b711-480442d0821c",
   "metadata": {},
   "source": [
    "## Samples visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf21bba-0330-4746-9d1b-2e3782f46390",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_random_files(\n",
    "    video_paths=msrvtt_dataset.train_video_paths,\n",
    "    video_annos=msrvtt_dataset.train_annos,\n",
    "    n_samples=config.vis_n_samples,\n",
    "    n_cols=config.vis_n_cols,\n",
    "    width=config.vis_width\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f96732-a446-4f6a-ba11-fbd68abac6dc",
   "metadata": {},
   "source": [
    "# Embeddings generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495e76c-b314-45a3-8bc3-e41f31a7d602",
   "metadata": {},
   "source": [
    "Each frame transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f5495-ef71-4ab0-b7b3-a5589ac250e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(\n",
    "        height=config.img_h, \n",
    "        width=config.img_w\n",
    "    ),\n",
    "    A.Normalize(\n",
    "        mean=config.norm_mean,\n",
    "        std=config.norm_std,\n",
    "    ),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136c34a-a3ce-4e26-8fe5-958d7f889e2d",
   "metadata": {},
   "source": [
    "Here is an example of how a video is splitted into individual frames at a certain frame interval and a transformation is performed on each frame (Here I applied only resizing and normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48af66d-bc7c-448c-96d5-781d4538cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = msrvtt_dataset.train_video_paths[28]\n",
    "video_name = video_path.name\n",
    "video_info = train_video_infos[video_name]\n",
    "print(video_info)\n",
    "\n",
    "video_dataset = VideoFrameDataset(\n",
    "    video_path=video_path, \n",
    "    frame_interval=config.frame_interval, \n",
    "    transform=transform,\n",
    ")\n",
    "print('Single video torch dataset len: ', len(video_dataset))\n",
    "\n",
    "frames_np = get_numpy_images(video_dataset.frames)\n",
    "\n",
    "visualize_frames(\n",
    "    frames_np,\n",
    "    n_cols=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3f97e-c811-4eed-aa57-d6581410197b",
   "metadata": {},
   "source": [
    "## Embeddings generation example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643167e3-0bf5-466b-acc5-9d093d6fdd0a",
   "metadata": {},
   "source": [
    "I decided to use CLIP ViT-B-32 vision encoder model from openclip to generate frame embeddings and after that aggregate frame embeddings to get single feature vector for each video.\n",
    "\n",
    "I generated embeddings for whole train dataset using **generate_embeddings.py** script, because it takes several hours that's why it is not convenient to do this in a jupyter notebook. Here I show the embedding generation for only one video. I saved embeddings for each video file, preserving temporary information. I did this to have the ability to change the frame embeddings' aggregation strategy without needing to recompute the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74a820-c7e8-485b-b349-45b215ebe804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(\n",
    "    model_type=config.model_type,\n",
    "    device=config.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c64f74-241c-48b2-9a8f-74fa88d57812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_path in tqdm(msrvtt_dataset.train_video_paths):\n",
    "    video_embeddings, video_frame_indexes = generate_video_embeddings(\n",
    "        video_path,\n",
    "        model,\n",
    "        transform,\n",
    "        batch_size=config.batch_size,\n",
    "        n_workers=config.n_workers,\n",
    "        frame_interval=config.frame_interval,\n",
    "        device=config.device\n",
    "    )\n",
    "    print('Embeddings shape: ', video_embeddings.shape)\n",
    "    print('Frame indexes shape: ', video_frame_indexes.shape) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac4de5-e7f5-44b0-be33-9775b3fd21c9",
   "metadata": {},
   "source": [
    "# Load computed embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154eeece-79cc-494d-9d31-5bb75d280d19",
   "metadata": {},
   "source": [
    "Pre-calculated embeddings for videos are loaded here and then they are aggregated along the time dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa4951-c79b-466d-8804-cd6c25a2218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = dataset_path / 'embeddings'\n",
    "embeddings_paths_list = list(embeddings_dir.glob('*.npz'))\n",
    "print('Number of embeddings files: ', len(embeddings_paths_list))\n",
    "\n",
    "all_embeddings = []\n",
    "all_video_names = []\n",
    "\n",
    "for embeddings_file_path in tqdm(embeddings_paths_list):\n",
    "    video_name = embeddings_file_path.stem + '.mp4'\n",
    "    embeddings = read_embeddings_and_aggregate(\n",
    "        embeddings_file_path,\n",
    "        agg_strategy=config.agg_strategy\n",
    "    )\n",
    "    all_embeddings.append(embeddings)\n",
    "    all_video_names.append(video_name)\n",
    "\n",
    "all_embeddings = np.concatenate(all_embeddings)\n",
    "all_video_names = np.array(all_video_names)\n",
    "print('All embeddings array shape: ', all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170901d-6870-4d6e-a715-309157a02fc8",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81481d56-e925-41d1-a5f9-235cce55c750",
   "metadata": {},
   "source": [
    "I decided to choose number of clusters equal to 20 because the paper describing the dataset says:\n",
    "\n",
    "*\"MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering a comprehensive list of 20 categories and a wide variety of video content.\"*\n",
    "\n",
    "However, it is easy to conduct a study based on clustering metrics (silhouette score ,Davies-Bouldin index etc) to select the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfca79-fe8b-484d-a9dc-347bf0a347c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=config.n_clusters, \n",
    "    random_state=config.random_seed, \n",
    "    n_init=\"auto\"\n",
    ").fit(all_embeddings)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "silhouette_avg = silhouette_score(all_embeddings, kmeans.labels_)\n",
    "print('Average silhouette score: ', silhouette_avg)\n",
    "\n",
    "davies_bouldin_avg = davies_bouldin_score(all_embeddings, kmeans.labels_)\n",
    "print('Average Davies-Bouldin score: ', davies_bouldin_avg)\n",
    "\n",
    "distances = cdist(\n",
    "    all_embeddings, \n",
    "    cluster_centers, \n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "result_dict = dict(\n",
    "    video_id=all_video_names, \n",
    "    cluster_id=labels\n",
    ")\n",
    "df = pd.DataFrame(result_dict) \n",
    "df.to_csv(save_dir/'video_cluster_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8888e0-3fd8-44eb-aa96-d7368fe5d683",
   "metadata": {},
   "source": [
    "# Cluster description generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0e54f-bcb4-40ce-8b6a-c14af9c89d66",
   "metadata": {},
   "source": [
    "Idea behind cluster descriptions generation is following:\n",
    "\n",
    "1) Select \"kw_k_nearest\" number of cluster samples nearest to the cluster center\n",
    "2) Each selected video has 20 text captions, let's make only one using summarization\n",
    "3) Group summarized video captions into one sentence and make keywords extraction using KeyBERT\n",
    "4) Filter resulting keywords based on score threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914cf1c-6c90-41a0-b0b9-3a0d7d2a6b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    device=config.device,\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8b4f9-8e2c-4ed3-af13-745b5c8cb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geterate_cluster_tags(\n",
    "    kw_model,\n",
    "    summarizer,\n",
    "    video_names,\n",
    "    video_annotations,\n",
    "    labels,\n",
    "    kw_k_nearest=20,\n",
    "    kw_top_n=10,\n",
    "    kw_score_threshold=0.2,\n",
    "    summarizer_max_length=100,\n",
    "    summarizer_min_length=20\n",
    "):\n",
    "    cluster_keywords = {}\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    for i in tqdm(range(n_clusters)):\n",
    "        cluster_indices = np.where(labels == i)[0]\n",
    "        cluster_distances = distances[cluster_indices, i]\n",
    "        nearest_indices = np.argsort(cluster_distances)[:kw_k_nearest]\n",
    "        nearest_sample_indices = [cluster_indices[idx] for idx in nearest_indices]\n",
    "        nearest_video_names = video_names[nearest_sample_indices]\n",
    "        \n",
    "        video_summaries = []\n",
    "        for video_name in nearest_video_names:\n",
    "            video_combined_caption = '\\n'.join(video_annotations[video_name])\n",
    "            video_sumary = summarizer(\n",
    "                video_combined_caption, \n",
    "                max_length=summarizer_max_length, \n",
    "                min_length=summarizer_min_length, \n",
    "                do_sample=False\n",
    "            )[0]['summary_text']\n",
    "            video_summaries.append(video_sumary)\n",
    "        \n",
    "        cluster_combined_caption = '\\n'.join(video_summaries)    \n",
    "        keywords = kw_model.extract_keywords(\n",
    "            cluster_combined_caption,\n",
    "            top_n=kw_top_n,\n",
    "            keyphrase_ngram_range=(1, 2)\n",
    "        )\n",
    "\n",
    "        cluster_keywords[i] = [kw for kw, kw_score in keywords if kw_score>=kw_score_threshold]\n",
    "        print(f'Cluster {i} keywords: ', cluster_keywords[i])\n",
    "        \n",
    "    return cluster_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4634bb6-cf86-4fee-8f59-04b62f4f081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_annotations = msrvtt_dataset.train_annos\n",
    "\n",
    "cluster_keywords = geterate_cluster_tags(\n",
    "    kw_model,\n",
    "    summarizer,\n",
    "    video_names=all_video_names,\n",
    "    video_annotations=video_annotations,\n",
    "    labels=labels,\n",
    "    kw_k_nearest=config.kw_k_nearest,\n",
    "    kw_top_n=config.kw_top_n,\n",
    "    kw_score_threshold=config.kw_score_threshold,\n",
    "    summarizer_max_length=config.summarizer_max_length,\n",
    "    summarizer_min_length=config.summarizer_min_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b400350-f47f-4f4d-9e69-98fb9adbea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir / 'cluster_descriptions.json', 'w') as fp:\n",
    "    json.dump(cluster_keywords, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7d58c-f88e-4de3-a1d9-6971a41f79a6",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de049881-a303-46d8-937c-4014bf26c12b",
   "metadata": {},
   "source": [
    "In order to visually assess the quality of our clusters, let's project our embeddings onto a two-dimensional space usin TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08b555-2024-45b2-946f-df62d0ddff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    n_components=2, \n",
    "    # perplexity=50,\n",
    "    n_jobs=config.n_workers,\n",
    "    random_state=config.random_seed\n",
    ")\n",
    "\n",
    "embeddings_to_project = np.concatenate([\n",
    "    all_embeddings,\n",
    "    cluster_centers\n",
    "])\n",
    "\n",
    "embeddings_projected = tsne.fit_transform(embeddings_to_project)\n",
    "embeddings_projected_videos = embeddings_projected[:-config.n_clusters, :]\n",
    "embeddings_projected_cluster_centers = embeddings_projected[-config.n_clusters:, :]\n",
    "\n",
    "print('projected embeddings shape: ', embeddings_projected_videos.shape)\n",
    "print('projected cluster centers shape: ', embeddings_projected_cluster_centers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af34b7-5b53-45f4-9260-c08370cd2502",
   "metadata": {},
   "source": [
    "## Clusters TSNE plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a80cd-da7e-4d59-aff9-b4cd76e0c473",
   "metadata": {},
   "source": [
    "Let's visualize our 2D embedding projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8843df-b980-4b9a-a63b-91960ac0f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(\n",
    "    embeddings_projected_videos,\n",
    "    embeddings_projected_cluster_centers,\n",
    "    labels,\n",
    "    video_names=all_video_names,\n",
    "    annotations=msrvtt_dataset.train_annos,\n",
    "    show_only_first_annotation=True,\n",
    "    cluster_keywords=cluster_keywords,\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca8b36-2fa3-4628-ab48-3ad0c8e03e68",
   "metadata": {},
   "source": [
    "## Clusters count plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4802d72-c644-40c5-b08f-692a4b918288",
   "metadata": {},
   "source": [
    "Let's also look at the distribution of the number of videos in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7d30f-293b-4a80-a07a-14109654a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count_plot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9737495-4e29-48f0-88ad-e96eca4baade",
   "metadata": {},
   "source": [
    "# Example videos from cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db775353-47ac-407b-a729-8a15d4cad31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 0\n",
    "label_mask = np.array(labels) == cluster_id\n",
    "cluster_video_names = all_video_names[label_mask]\n",
    "cluster_video_paths = [dataset_path / 'videos/all/' / video_name for video_name in cluster_video_names]\n",
    "\n",
    "print('Cluster keywords: ', cluster_keywords[cluster_id])\n",
    "\n",
    "visualize_random_files(\n",
    "    video_paths=cluster_video_paths,\n",
    "    video_annos=msrvtt_dataset.train_annos,\n",
    "    n_samples=config.vis_n_samples,\n",
    "    n_cols=config.vis_n_cols,\n",
    "    width=config.vis_width\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ba771-988e-4d70-a48e-a11d42560bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
